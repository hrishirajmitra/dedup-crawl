# **Technical Report: Priority-Based Dynamic Crawler**

**Hrishiraj Mitra**

## **1\. Overview**

This report details the methodology for a modular, efficient crawling pipeline designed to analyze a dynamic website. The primary objectives were to:

1. Discover the complete structure of the site.  
2. Estimate the PageRank of each page.  
3. Track dynamic content changes (represented by node\_id).  
4. **Minimize the total number of page visits** to achieve these goals.

The solution is a two-phase Python pipeline. Phase 1 performs a one-time discovery crawl to build a graph. Phase 2 enters an intelligent, long-running monitoring loop that uses PageRank to inform a priority-based visiting schedule, drastically reducing visit frequency for low-importance pages.

## **2\. Methodology**

The pipeline is segregated into six modules (main.py, crawler.py, fetcher.py, pagerank.py, visualizer.py, config.py) to separate I/O, state management, and algorithmic logic.

### **2.1. Phase 1: Site Discovery & Graph Construction**

This phase builds the initial model of the website.

* **Dynamic Start Page:** The pipeline does not assume a hardcoded start page. The fetcher.py module first queries the server's root (/) to parse the HTML and discover the page\_id of the main portal, which serves as the starting point for the crawl.  
* **Graph Construction (BFS):** A Breadth-First Search (BFS) is initiated from the start page. To minimize visits and prevent loops, a Set (self.visited) tracks all visited pages. A page is fetched and processed exactly once during this phase.  
* **Sparse Data Structure:** The site topology is stored as an **Adjacency List** (Dict\[str, List\[str\]\]). This is a memory-efficient sparse representation, ensuring that algorithms only iterate over existing links, not a large, empty N x N matrix.  
* **HTML Parsing:** The fetcher.py module uses BeautifulSoup to parse the raw HTML of each page, extracting the page\_id, current node\_id, node\_history, and all outgoing\_links. This data is returned as a clean dictionary, abstracting the parsing logic from the crawler.

### **2.2. Phase 2: PageRank Estimation**

Once the graph is built, the relative importance of each page is calculated.

* **Algorithm:** The standard iterative **Power Method** is used to calculate PageRank scores for all discovered nodes.  
* **Gauss-Seidel Optimization:** To improve convergence speed, the PageRank algorithm is implemented with the **Gauss-Seidel** optimization. Instead of using all PageRank values from the previous iteration (Jacobi method), this implementation uses newly computed values *within the same iteration* as soon as they become available. This allows information to propagate faster through the graph, often leading to convergence in fewer iterations.

### **2.3. Phase 3: Priority-Based Monitoring (Minimizing Visits)**

This is the core of the efficiency strategy. Instead of a "naive" approach (re-crawling all pages), this phase uses the calculated PageRank to minimize visits.

* **1\. Priority Bucketing:** After the initial PageRank calculation, all visited pages are sorted by their rank. They are then divided into three priority buckets using numpy.quantile for robust statistical splits:  
  * **P1 (High):** Top 20% of pages (e.g., rank \>= 80th percentile).  
  * **P3 (Low):** Bottom 30% of pages (e.g., rank \< 30th percentile).  
  * **P2 (Medium):** All remaining pages (the middle 50%).  
* **2\. Differential Update Frequencies:** The main.py scheduler enters an infinite loop that checks these buckets at different intervals, defined in the config. For example:  
  * **P1 pages** are checked every **10 seconds**.  
  * **P2 pages** are checked every **60 seconds**.  
  * P3 pages are checked every 300 seconds.  
    This "Priority-Based Visiting" strategy (from the provided advanced methods) ensures that high-importance pages are monitored closely for changes, while low-importance, (likely)-static pages are visited 30 times less frequently, drastically reducing the overall visit count.  
* **3\. Stateful Update Tracking:** The Crawler class maintains a node\_states dictionary (Dict\[page\_id, node\_id\]) to store the last known node\_id for every page. When a page is visited during a monitoring sweep, its new node\_id is compared to the stored value. If they differ, an "UPDATE" is logged.

### **2.4. Live Visualization**

To provide insight into the pipeline's state, the visualizer.py module generates two separate, periodically-updated HTML files. This separation ensures reliability (the dashboard always loads) and interactivity.

* **dashboard.html:** A lightweight, static HTML dashboard. It displays high-level statistics (total pages, total links, total updates) and leaderboards (Top 5 by PageRank, Top 5 by Activity/Updates).  
* **graph.html:** A standalone, fully interactive graph generated by pyvis. This file is linked from the dashboard. It encodes key analytics visually:  
  * **Node Size (Log-Scaled):** Represents PageRank (larger \= higher rank).  
  * **Node Color (Heatmap):** Represents Update Frequency (Blue: static \-\> Red: frequent updates).

## **3\. How to Run**

### **3.1. Prerequisites**

1. **Docker:** Must be installed to run the web server.  
2. **Python 3.x:** Required to run the crawler script.  
3. **Python Libraries:** Install all required libraries using the provided requirements.txt:  
   pip install \-r requirements.txt

### **3.2. Running the Pipeline**

You will need two separate terminals.

**Terminal 1: Run the Web Server**

1. Load the Docker image (one-time operation):  
   docker load \-i crawling\_assignment-1.0-amd64.tar

2. Run the Docker container to start the server:  
   docker run \--rm \-p 3000:3000 \--read-only \--tmpfs /tmp:rw,noexec,nosuid \--cap-drop ALL \--security-opt no-new-privileges \--pids-limit 128 \--memory 256m crawling\_assignment:1.0

   The server is now running at http://localhost:3000.

**Terminal 2: Run the Crawler**

1. Navigate to the directory containing the Python files (main.py, etc.).  
2. Execute the main script:  
   python main.py

### **3.3. Viewing the Output**

1. The script will start logging its progress to the console.  
2. After the initial discovery phase (approx. 10-30 seconds), the script will log:  
   \--- Your dashboard is at 'dashboard.html' \---  
   \--- Your graph is at 'graph.html' \---

3. Open **dashboard.html** in your web browser.  
4. You will see the statistics. Click the link at the top to open **graph.html** in a new tab.  
5. Both files will be automatically updated by the script every 5 minutes (or as configured in main.py). Simply refresh your browser to see the latest data.